import * as fs from 'fs';
import * as path from 'path';
import * as cp from 'child_process';

// --- Types & Interfaces ---

type Severity = 'HIGH' | 'MEDIUM' | 'LOW';

interface Finding {
  severity: Severity;
  kind: 'path' | 'content';
  category: string;
  detail: string;
  evidence?: string;
}

interface Pattern {
  severity: Severity;
  kind: 'path' | 'content';
  category: string;
  re: RegExp;
  detail?: string; // For path signals
  why?: string;    // For prose/code patterns
}

// --- Setup ---

const repo: string = process.cwd();
const head: string = cp.execSync("git rev-parse HEAD", { encoding: "utf8" }).trim();
const files: string[] = cp.execSync("git ls-files", { encoding: "utf8" })
  .split(/\r?\n/)
  .filter(Boolean);

const extOf = (p: string): string => path.extname(p).toLowerCase();
const baseOf = (p: string): string => path.basename(p);

const isLikelyBinary = (buf: Buffer): boolean => {
  if (!buf || buf.length === 0) return false;
  const sample = buf.subarray(0, Math.min(buf.length, 8000));
  for (let i = 0; i < sample.length; i++) {
    if (sample[i] === 0) return true;
  }
  return false;
};

const isTestFile = (p: string): boolean => 
  /(^|\/)(__tests__|tests)(\/|$)/.test(p) || /\.(test|spec)\./.test(p) || /e2e\//.test(p);

const isDocsLike = (p: string): boolean => {
  const ext = extOf(p);
  return ext === ".md" || ext === ".txt" || p.startsWith("docs/") || p.startsWith(".github/");
};

const isCodeLike = (p: string): boolean => /\.(js|jsx|ts|tsx|cjs|mjs)$/.test(extOf(p));

const binaryExts: Set<string> = new Set([".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg", ".heic", ".db", ".crt"]);

// --- Patterns ---

const fileNameSignals: Pattern[] = [
  {
    severity: "HIGH",
    kind: "path",
    category: "Committed build output / generated artifacts",
    re: /(^|\/)(dist\/|playwright-report\/)/,
    detail: "Build outputs / reports are usually not committed (unless this repo intentionally vendors artifacts).",
  },
  {
    severity: "HIGH",
    kind: "path",
    category: "Committed database / runtime state",
    re: /(^|\/)(server\/server_photos\.(db|json)$|server\/working\/dev\.db$)/,
    detail: "Committing DB/state files is atypical for senior repo hygiene and can indicate iteration artifacts.",
  },
  {
    severity: "MEDIUM",
    kind: "path",
    category: "Large volume of iterative fix logs / work notes committed",
    re: /(^|\/)(docs\/history\/.*\.md$|.*_FIX_.*\.md$|.*_REPORT.*\.md$|.*_SUMMARY\.md$|.*_LOG\.md$|PROBLEM_LOG\.md$|docs\/error-handling-audit\.md$|docs\/DOCUMENTATION_AUDIT\.md$)/,
    detail: "Large quantities of narrative fix logs/checklists are more common in AI-assisted iteration than curated repos.",
  },
  {
    severity: "MEDIUM",
    kind: "path",
    category: "AI/editor tooling provenance doc committed",
    re: /(^|\/)(readme-\d{4}-\d{2}-\d{2}.*\.md$|\.github\/copilot-instructions\.md$)/,
    detail: "Docs explicitly tied to AI/editor tooling are strong signals of AI-assisted workflows.",
  },
  {
    severity: "LOW",
    kind: "path",
    category: "Ad-hoc outputs committed",
    re: /(^|\/)(git-history\.txt$|test_output\.txt$|file-picker-files\.txt$)/,
    detail: "Committed ad-hoc outputs are a mild repo-hygiene smell.",
  },
];

const prosePatterns: Pattern[] = [
  {
    severity: "HIGH",
    kind: "content",
    category: "Explicit AI tool authorship / provenance",
    re: /(generated by chatgpt|written by chatgpt|created with chatgpt|generated by cursor|cursor ai)/gi,
    why: "Direct provenance statements are strong indicators of AI-written content.",
  },
  {
    severity: "HIGH",
    kind: "content",
    category: "Assistant self-reference / LLM disclaimer",
    re: /(\bas an ai\b|\bI am an AI\b|\blanguage model\b|\bI (?:cannot|can(?:\u0027|\u2019)?t)\b)/gi,
    why: "Self-referential assistant phrasing rarely belongs in repo docs.",
  },
  {
    severity: "MEDIUM",
    kind: "content",
    category: "Copilot/AI remediation attribution",
    re: /(remediated by:\s*github copilot|fixed by:\s*github copilot|senior .* mode)/gi,
    why: "Attribution lines like this are unusual in typical engineering docs and suggest AI-assisted generation.",
  },
  {
    severity: "LOW",
    kind: "content",
    category: "Prompt-like all-caps directives",
    re: /(\bCRITICAL\b:?|\bIMPORTANT\b:?|\bMUST\b|\bNEVER\b|\bALWAYS\b)/g,
    why: "All-caps prompt-like instruction blocks often originate from agent prompts.",
  },
];

const codeDirectivePatterns: Pattern[] = [
  {
    severity: "MEDIUM",
    kind: "content",
    category: "Prompt/agent-like directives embedded",
    re: /(\bCRITICAL\b:?|\bMUST\b|\bNEVER\b|\bALWAYS\b).{0,160}/g,
    why: "Prompt-style directives inside code/comments can be a smell of LLM-driven edits or prompt copy/paste.",
  },
];

const secretPatterns: Pattern[] = [
  {
    severity: "HIGH",
    kind: "content",
    category: "Env/workflow contains key-like fields",
    re: /(OPENAI_API_KEY\s*=|SUPABASE_SERVICE_ROLE_KEY\s*=|SUPABASE_ANON_KEY\s*=|VITE_SUPABASE_ANON_KEY\s*=|DATABASE_URL\s*=)/g,
    why: "Env/workflow files with key-like fields are sensitive; committing them (even placeholders) often correlates with rushed/automated iteration.",
  },
];

// --- Helper Functions ---

function lineEvidence(content: string, idx: number): { lineNo: number; line: string } {
  const before = content.slice(0, idx);
  const lineNo = before.split(/\r?\n/).length;
  const lines = content.split(/\r?\n/);
  const line = lines[lineNo - 1] || "";
  return { lineNo, line: line.trim().slice(0, 260) };
}

function isEnvOrWorkflowFile(rel: string): boolean {
  const base = baseOf(rel);
  if (base.startsWith(".env")) return true;
  if (rel.startsWith(".github/workflows/") && /\.(yml|yaml)$/i.test(rel)) return true;
  return false;
}

// --- Execution Loop ---

const byFile: Map<string, Finding[]> = new Map();
for (const f of files) byFile.set(f, []);

for (const rel of files) {
  // path-level findings
  for (const s of fileNameSignals) {
    if (s.re.test(rel)) {
      byFile.get(rel)?.push({
        severity: s.severity,
        kind: 'path',
        category: s.category,
        detail: s.detail || ""
      });
    }
  }

  const abs = path.join(repo, rel);
  let buf: Buffer;
  try {
    buf = fs.readFileSync(abs);
  } catch (e: any) {
    byFile.get(rel)?.push({
      severity: "MEDIUM",
      kind: 'path',
      category: "Unreadable file",
      detail: "Failed to read: " + e.message
    });
    continue;
  }

  const ext = extOf(rel);
  if (binaryExts.has(ext) || isLikelyBinary(buf)) {
    byFile.get(rel)?.push({
      severity: "LOW",
      kind: 'path',
      category: "Binary file",
      detail: "Binary or non-text content; line-level scan skipped."
    });
    continue;
  }

  const content = buf.toString("utf8");

  if (isEnvOrWorkflowFile(rel)) {
    for (const p of secretPatterns) {
      p.re.lastIndex = 0;
      let hits = 0;
      let m: RegExpExecArray | null;
      while ((m = p.re.exec(content)) && hits < 15) {
        const ev = lineEvidence(content, m.index);
        byFile.get(rel)?.push({
          severity: p.severity,
          kind: p.kind,
          category: p.category,
          detail: p.why || "",
          evidence: "L" + ev.lineNo + ": " + ev.line
        });
        hits++;
      }
    }
  }

  if (isDocsLike(rel)) {
    for (const p of prosePatterns) {
      p.re.lastIndex = 0;
      let hits = 0;
      let m: RegExpExecArray | null;
      while ((m = p.re.exec(content)) && hits < 10) {
        const ev = lineEvidence(content, m.index);
        byFile.get(rel)?.push({
          severity: p.severity,
          kind: p.kind,
          category: p.category,
          detail: p.why || "",
          evidence: "L" + ev.lineNo + ": " + ev.line
        });
        hits++;
      }
    }
  }

  if (isCodeLike(rel) && !isTestFile(rel)) {
    for (const p of codeDirectivePatterns) {
      p.re.lastIndex = 0;
      let hits = 0;
      let m: RegExpExecArray | null;
      while ((m = p.re.exec(content)) && hits < 10) {
        const ev = lineEvidence(content, m.index);
        byFile.get(rel)?.push({
          severity: p.severity,
          kind: p.kind,
          category: p.category,
          detail: p.why || "",
          evidence: "L" + ev.lineNo + ": " + ev.line
        });
        hits++;
      }
    }
  }
}

// --- Sorting & Output ---

const severityRank: Record<Severity, number> = { HIGH: 3, MEDIUM: 2, LOW: 1 };

function sortFindings(list: Finding[]): Finding[] {
  return list
    .slice()
    .sort((a, b) => {
      const r = (severityRank[b.severity] || 0) - (severityRank[a.severity] || 0);
      if (r) return r;
      const ea = a.evidence || "";
      const eb = b.evidence || "";
      if (ea !== eb) return ea.localeCompare(eb);
      return (a.category || "").localeCompare(b.category || "");
    });
}

const now = new Date().toISOString();
let out = "";
out += "# Per-file AI Smell Audit (all tracked files)\n\n";
out += "- Repo: React-Photo-App\n";
out += "- Commit: " + head + "\n";
out += "- Generated: " + now + "\n";
out += "- Tracked files audited: " + files.length + "\n\n";
out += "## How to use\n\n";
out += "- Each section is a **file**.\n";
out += "- Each bullet with `L###:` is a **line number inside that file**.\n";
out += "- In VS Code/Cursor: open the file, press **Ctrl+G**, type the number.\n\n";

for (const rel of files) {
  out += "## " + rel + "\n\n";
  const items = sortFindings(byFile.get(rel) || []);

  const seen = new Set<string>();
  const deduped: Finding[] = [];
  for (const it of items) {
    const key = [it.severity, it.category, it.evidence || "", it.detail || ""].join("||");
    if (seen.has(key)) continue;
    seen.add(key);
    deduped.push(it);
  }

  if (!deduped.length) {
    out += "- No AI-smell findings triggered by the current heuristics.\n\n";
    continue;
  }

  for (const it of deduped) {
    out += "- **" + it.severity + " / " + it.category + "**";
    out += it.evidence ? " â€” " + it.evidence : " (path-level)";
    out += "\n";
    out += "   - " + it.detail + "\n";
  }

  out += "\n";
}

fs.writeFileSync(path.join(repo, "AI_CODE_REVIEW.md"), out, "utf8");
console.log("Wrote AI_CODE_REVIEW.md in per-file format");
